{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "$$ \\LaTeX \\text{ command declarations here.}\n",
    "\\newcommand{\\R}{\\mathbb{R}}\n",
    "\\renewcommand{\\vec}[1]{\\mathbf{#1}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# plotting\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt;\n",
    "\n",
    "# scientific\n",
    "import numpy as np;\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.learning_curve import learning_curve\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "# math\n",
    "from __future__ import division\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# EECS 545:  Machine Learning\n",
    "## Lecture 13:  Information Theory and Exponential Families\n",
    "* Instructor:  **Jacob Abernethy**\n",
    "* Date:  March 7, 2016\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "This lecture draws from following resources:\n",
    "* Chapter 9, \"Generalized linear models and the exponential family\" of *Machine Learning, A Probabilistic- Perspective* (Kevin P. Murphy), pp 281-289.\n",
    "\n",
    "\n",
    "* Recommended Additional Reading:\n",
    "    * Statistical Methods for Signal Processing, by Alfred O. Hero: Sections 3.5, 4.4.2\n",
    "    * MLAPP (Murphy), Section 2.5.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline\n",
    "<Info Theory>\n",
    "\n",
    "* Exponential Family\n",
    "    * Introduction\n",
    "    * MLE for exponential family"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exponential Family: Introduction\n",
    "\n",
    "* We have seen many distributions.\n",
    "    * Bernoulli\n",
    "    * Gaussian\n",
    "    * Exponential\n",
    "    * Gamma \n",
    "    \n",
    "    \n",
    "* It so happens that most of them belong to same family of distributions, ** exponential family **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction(2) \n",
    "\n",
    "\n",
    "* **Why is this important**?\n",
    "    * only family of distributions with finite-sized sufficient statistics* (under certain regularity conditions).\n",
    "    * only family of distributions for which conjugate priors exist.\n",
    "    * makes the least set of assumptions subject to some user-chosen constraints.\n",
    "    * core of generalized linear models and variational inference.\n",
    "    \n",
    "\n",
    "*Further reading: Section 3.5, Statistical Methods for Signal Processing, by Alfred O. Hero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Definition\n",
    "\n",
    "A pdf or pmf $p(x|\\theta)$, for $x = (x_1, ...., x_m)\\in \\mathcal{X}^m $ and $\\theta \\in \\Theta \\subseteq \\R^d$, is said to be in the **exponential family**, if it can be expressed in the form of \n",
    "$$ \n",
    "\\begin{align}\n",
    "p (x|\\theta)\n",
    "&= \\frac{1}{Z(\\theta)} h(x) exp[\\theta^T \\phi(x)] \\\\\n",
    "&= h(x) exp[\\theta^T \\phi(x) - A(\\theta)]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where\n",
    "$$\n",
    "\\begin{align}\n",
    "Z(\\theta) &= \\int_{\\mathcal{X}^m} h(x)exp[\\theta^T \\phi(x)] dx \\\\\n",
    "A(\\theta) &= log Z(\\theta)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Definition (2)\n",
    "\n",
    "Explanation of Notation\n",
    "\n",
    "* $\\theta$: natural(canonical) parameters.\n",
    "* $\\phi(x) \\in \\R^d$: vector of sufficient statistics.\n",
    "* $Z(\\theta)$: Partition function\n",
    "* $A(\\theta)$: Cumulant function (log-partition function)\n",
    "* $h(x)$: scaling constant\n",
    "\n",
    "\n",
    "If $\\phi(x) = x$, we say it is a **natural exponential family**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Definition (3)\n",
    "\n",
    "We could generalize the last equation by replacing $\\theta$ with $\\eta(\\theta)$, where $\\eta$ is a function that maps the parameters $\\theta$ to the canonical paramaters $\\eta = \\eta(\\theta)$.\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "p (x|\\theta)\n",
    "&= h(x) exp[\\eta(\\theta)^T \\phi(x) - A(\\eta(\\theta))]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "* $dim(\\theta) < dim(\\eta(\\theta))$: ** curved exponential family** (more sufficient statistics than parameters)\n",
    "* $\\eta(\\theta) = \\theta $: **canonical form** of model (default assumption in this lecture, unless stated otherwise)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Examples\n",
    "\n",
    "#### Bernoulli\n",
    "\n",
    "Bernoulli $x \\in {0,1}$ can be written as:\n",
    "$$ Ber(x|\\mu) = \\mu^x(1-\\mu)^{1-x} = exp(xlog(\\mu) + (1-x)log(1-\\mu) = exp(\\phi(x)^T \\theta) $$\n",
    "\n",
    "where $\\mu = P(x=1)$, $\\phi(x) = [\\mathcal{I}(x=0), \\mathcal{I}(x=1)]$ and $\\theta = [log(\\mu), log(1-\\mu)]$.\n",
    "\n",
    "* This representation is over-complete\n",
    "    * linear dependence between features: $1^T \\phi(x) = \\mathcal{I}(x=0)+ \\mathcal{I}(x=1) = 1$\n",
    "    * As a result, $\\theta$ is not uniquely determined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Bernoulli(2)\n",
    "\n",
    "* It is common to require that the representation be **minimal**, which means there is a unique $\\theta$ associated with the distribution. Hence, define\n",
    "\n",
    "$$ Ber(x|\\mu) = (1-\\mu) exp[xlog(\\frac{\\mu}{1-\\mu})] $$\n",
    "\n",
    "* $\\phi(x) = x$\n",
    "* $\\theta = log (\\frac{\\mu}{1-\\mu})$: log-odds ratio\n",
    "* $Z = 1/(1-\\mu)$\n",
    "* $ \\mu = sigmoid(\\theta) = \\frac{1}{1+e^{-\\theta}} $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Examples(2)\n",
    "#### Univariate Gaussian \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{N}(x|\\mu, \\sigma^2)\n",
    "&= \\frac{1}{(2\\pi\\sigma^2)^{1/2}}exp[-\\frac{1}{2\\sigma^2}(x-\\mu)^2] \\\\\n",
    "&= \\frac{1}{(2\\pi\\sigma^2)^{1/2}}exp[-\\frac{x^2}{2\\sigma^2}+\\frac{\\mu}{\\sigma^2} - \\frac{\\mu^2}{2\\sigma^2}] \\\\\n",
    "&= \\frac{1}{Z(\\theta)}exp(\\theta^T \\phi(x))\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "* $\\theta = [\\frac{\\mu}{\\sigma^2}, \\frac{-1}{2\\sigma^2}]^T $$\\hspace{2em}$ $\\phi(x) = [x, x^2]^T$\n",
    "* $Z(\\mu, \\sigma^2) = \\sqrt{2\\pi}\\sigma exp(\\mu^2/(2\\sigma^2))$\n",
    "* $A(\\theta) = \\frac{-\\theta_1^2}{4\\theta_2} - \\frac{1}{2}log(-2\\theta_2) - \\frac{1}{2}log(2\\pi) $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Non-examples\n",
    "\n",
    "Not all distributions of interest belong to the exponential family.\n",
    "\n",
    "* Uniform distribution $X \\sim Uni(a,b)$: support depends upon parameters\n",
    "\n",
    "* Student t-distribution*: cannot be represented in required form.\n",
    "\n",
    "\n",
    "\n",
    "* *Further Readings: \n",
    "    1. Section 2.5.3, MLAPP (Murphy) \n",
    "    2. Wiki article (https://en.wikipedia.org/wiki/Student%27s_t-distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Log-Partition Function\n",
    "\n",
    "Derivatives of the log partition function can be used to generate **cumulants** of the sufficient statistics ((Hence, the name **cumulant function**)\n",
    "\n",
    "* Moments: $\\mathbf{E}(X)$, $\\mathbf{E}(X^2)$\n",
    "* Cumulants: $\\mathbf{E}(X)$, $Var(X)$\n",
    "\n",
    "\n",
    "$A(\\theta)$ is **convex** function. Proof for 1-parameter distribution follows. \n",
    "    *Try it yourself*: generalize the proof to a K-parameter distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Proof of Convexity: First derivative\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{dA}{d\\theta}\n",
    "&= \\frac{d}{d\\theta}(log \\int exp(\\theta\\phi(x))h(x)dx) \\\\\n",
    "&= \\frac{\\frac{d}{d\\theta} \\int exp(\\theta\\phi(x))h(x)dx)}{\\int exp(\\theta\\phi(x))h(x)dx)} \\\\\n",
    "&= \\frac{\\int \\phi(x)exp(\\theta\\phi(x))h(x)dx}{exp(A(\\theta)} \\\\\n",
    "&= \\int \\phi(x) exp[\\theta\\phi(x)-A(\\theta)] h(x) dx \\\\\n",
    "&= \\int \\phi(x) p(x) dx \\\\\n",
    "&= \\mathbf{E}[\\phi(x)]\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Proof of Convexity: Second derivative\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{d^2A}{d\\theta^2}\n",
    "& = \\int \\phi(x)exp[\\theta \\phi(x) - A(\\theta)] h(x) (\\phi(x) - A'(\\theta)) dx \\\\\n",
    "& = \\int \\phi(x) p(x) (\\phi(x) - A'(\\theta))dx \\\\\n",
    "& = \\int \\phi^2(x) p(X) dx - A'(\\theta) \\int \\phi(x)p(x)dx \\\\\n",
    "& = \\mathbf{E}[\\phi^2(x)] - \\mathbf{E}[\\phi(x)]^2  \\hspace{2em}   (\\because A'(\\theta) = \\mathbf{E}[\\phi(x)])  \\\\ \n",
    "& = var(\\phi(x))\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Proof of Convexity: Second derivative(2)\n",
    "\n",
    "For multi-variate case, we have \n",
    "\n",
    "$$ \\frac{\\partial^2A}{\\partial\\theta_i \\partial\\theta_j} = \\mathbf{E}[\\phi_i(x)\\phi_j(x)] - \\mathbf{E}[\\phi_i(x)]\\mathbf{E}[\\phi_j(x)]$$\n",
    "\n",
    "and hence,\n",
    "$$ \\nabla^2A(\\theta) = cov[\\phi(x)] $$\n",
    "\n",
    "Since covariance is positive definite, we have $A(\\theta)$ convex as required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Log Partition Example\n",
    "#### Bernoulli\n",
    "\n",
    "* $ A(\\theta) = log(1+e^\\theta)$ \n",
    "* $$ \\mathbf{E}[\\phi(x)] = \\frac{dA}{d\\theta} = \\frac{e^\\theta}{1+ e^\\theta} = sigmoid(\\theta) = \\mu $$\n",
    "\n",
    "* $$\n",
    "\\begin{align}\n",
    "var[\\phi(x)] \\\n",
    "&= \\frac{d}{d\\theta}(1+e^{-\\theta})^{-1} = (1+e^{-\\theta})^{-2}\\cdot e^{-\\theta} \\\\\n",
    "&= \\frac{e^{-\\theta}}{1+e^{-\\theta}}\\cdot \\frac{1}{1+e^{-\\theta}} = \\frac{1}{1+e^\\theta}\\cdot\\frac{1}{1+e^{-\\theta}} \\\\\n",
    "&= (1-\\mu)\\mu\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MLE for exponential family\n",
    "\n",
    "#### General form of Likelihood function for Exponential Family\n",
    "\n",
    "$$ p(\\mathcal{D}|\\theta) = [\\prod_{i=1}^{N}h(x_i)]g(\\theta)^N exp(\\eta(\\theta)^T[\\sum_{i=1}^{N}\\phi(x_i)]) $$\n",
    "\n",
    "* Sufficient statistics:\n",
    "    * N\n",
    "    * $\\phi(\\mathcal{D}) = [\\sum_{i=1}^{N}\\phi_1(x_i), \\dots , \\sum_{i=1}^{N}\\phi_k(x_i)]$\n",
    "\n",
    "\n",
    "* Example:\n",
    "    * Bernoulli: N, $\\phi = [\\sum_i \\mathcal{I}(x_i=1)]$\n",
    "    * Univariate Guassian: N, $\\phi = [\\sum_i x_i, \\sum_i x_i^2]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pitman-Koopman-Darmois Theorem \n",
    "\n",
    "Among families of probability distributions whose domain does not vary with the parameter being estimated, only in exponential families is there a sufficient statistic whose dimension remains bounded as sample size increases.\n",
    "\n",
    "Suppose $X_n, n = 1, 2, 3, \\dots$ are independent identically distributed random variables whose distribution is known to be in some family of probability distributions. Only if that family is an exponential family is there a (possibly vector-valued) sufficient statistic $T(X_1, \\dots, X_n)$ whose number of scalar components does not increase as the sample size $n$ increases.\n",
    "\n",
    "* **Sufficiency of statistic restricts the type of distribution.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### PKD Theorem (2)\n",
    "\n",
    "* One of the conditions required in this theorem is that the support of the distribution not be dependent on the parameter.\n",
    "    * Example: \n",
    "        * $p(x|\\theta) = U(x|\\theta) = \\frac{1}{\\theta}\\mathcal{I}(0 \\leq x \\leq \\theta)$\n",
    "        * Likelihood: $ p(\\mathcal{D}|\\theta) = \\theta^{-N}\\mathcal{I}(0 \\leq max_i\\{x_i\\} \\leq \\theta) $\n",
    "        * sufficient statistics: $N$ and $s(\\mathcal{D}) = max_i \\{x_i\\}$\n",
    "        * Finite in size, but support depends on $\\theta$ and hence, not in exponential family."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Computing MLE for Canonical model\n",
    "\n",
    "* Given $N$ iid data points $\\mathcal{D} = (x_1, \\dots , x_N))$\n",
    "\n",
    "\n",
    "* log-likelihood: $ log p(\\mathcal{D}|\\theta) = \\theta^T \\phi(\\mathcal{D}) - N A(\\theta) $\n",
    "\n",
    "    * $- A(\\theta)$ is concave in $\\theta$.\n",
    "    * $\\theta^T\\phi(\\mathcal{D})$ is linear in $\\theta$.\n",
    "    * Consequently, log-likelihood is **concave** and achieves global maximum.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Method of Moments (MoM)\n",
    "\n",
    "* To derive MLE, use the fact that derivative of log-partition function yields $\\mathbf{E}[\\phi(x)]$.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla_\\theta log p(\\mathcal{D}|\\theta) = 0\\\\\n",
    "\\implies \\phi(\\mathcal{D}) - N \\mathbf{E}[\\phi(X)] = 0\\\\\n",
    "\\implies \\mathbf{E}[\\phi(X)] = \\frac{\\phi (\\mathcal{D})}{N}\\\\\n",
    "\\implies \\mathbf{E}[\\phi(X)] = \\frac{1}{N}\\sum_{i=1}^{N}\\phi(x_i)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "* *The sample of the sufficient statistics must equal the model’s theoretical expected sufficient statistics*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Method of Moments (2)\n",
    "\n",
    "* This is called **moment matching** or method of moments*.\n",
    "\n",
    "* Example: Bernoulli Distribution\n",
    "    * $\\phi(x) = \\mathcal{I}(X=1)$\n",
    "    * $\\mathbf{E}[\\phi(X)] = p(X=1) = \\hat \\mu = \\frac{1}{N} \\sum_{i=1}^{N} \\mathcal{I}(x_i=1)$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "*Further Reading: Section 4.4.2, Statistical Methods for Signal Processing, by Alfred O. Hero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bayes for Exponential Family\n",
    "\n",
    "* Fact: Exact Bayesian analysis is considerably simplified if the prior is conjugate to the likelihood.\n",
    "\n",
    "* Simply, this means that prior $p(\\mathcal{D}|\\tau)$ has the same form as likelihood $p(\\mathcal{D}|\\theta)$.\n",
    "\n",
    "* This requires likelihood to have finite sufficient statistics ,such that $p(\\mathcal{D}|\\theta) = p(s(\\mathcal{D})|\\theta)$\n",
    "\n",
    "* Hence, Exponential family to the rescue!\n",
    "\n",
    "\n",
    "**Running Example**: Bernoulli distribution, $Ber(x|\\theta) = \\theta^x (1-\\theta)^{1-x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Likelihood\n",
    "\n",
    "Likelihood: \n",
    "$$ p(\\mathcal{D}|\\theta) \\propto g(\\theta)^N exp[\\eta(\\theta)^T s_N]\\\\\n",
    "s_N = \\sum_{i=1}^{N}s(x_i)$$\n",
    "\n",
    "In terms of canonical parameters:\n",
    "$$ p(\\mathcal{D}|\\eta) \\propto exp[N\\eta^T \\bar{s} -N A(\\eta)] \\\\\n",
    "\\bar s = \\frac{1}{N}s_N $$\n",
    "\n",
    "\n",
    "**Example**:\n",
    "$$p(\\mathcal{D}|\\theta) = (1-\\theta)^N exp[log(\\frac{\\theta}{1-\\theta})\\sum_i x_i]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Prior\n",
    "\n",
    "Prior: \n",
    "$$ p(\\theta| \\nu_0, \\tau_0) \\propto g(\\theta)^{\\nu_0} exp[\\eta(\\theta)^T \\tau_0] $$\n",
    "\n",
    "* Denote $ \\tau_0 = \\nu_0 \\bar{\\tau}_0$ to separate out the size of the prior pseudo-data, $\\nu_0$ , from the mean of the sufficient statistics on this pseudo-data, $\\tau_0$ . Hence,\n",
    "\n",
    "$$ p(\\theta| \\nu_0, \\bar \\tau_0) \\propto exp[\\nu_0\\eta^T \\bar \\tau_0  - \\nu_0 A(\\eta)] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Prior(2)\n",
    "\n",
    "**Example**: \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(\\theta| \\nu_0, \\tau_0) \n",
    "&\\propto (1-\\theta)^{\\nu_0} exp[log(\\frac{\\theta}{1-\\theta}\\tau_0)] \\\\\n",
    "&= \\theta^{\\nu_0}(1-\\theta)^{\\nu_0 - \\tau_0}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Define $\\alpha = \\tau_0 +1 $ and $\\beta = \\nu_0 - \\tau_0 +1$ to see that this is a *beta distribution*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Posterior\n",
    "\n",
    "* Posterior: \n",
    "$$ p(\\theta|\\mathcal{D}) = p(\\theta|\\nu_N, \\tau_N) = p(\\theta| \\nu_0 +N, \\tau_0 +s_N) $$\n",
    "\n",
    "* Note that we obtain hyper-parameters by adding. Hence,\n",
    "\n",
    "$$ \\begin{align}\n",
    "p(\\eta|\\mathcal{D})\n",
    "&\\propto exp[\\eta^T (\\nu_0 \\bar\\tau_0 + N \\bar s) - (\\nu_0 + N) A(\\eta) ] \\\\\n",
    "&= p(\\eta|\\nu_0 + N, \\frac{\\nu_0 \\bar\\tau_0 + N \\bar s}{\\nu_0 + N}\n",
    "\\end{align}$$\n",
    "\n",
    "* *posterior hyper-parameters are a convex combination of the prior mean hyper-parameters and the average of the sufficient statistics.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Posterior(2)\n",
    "\n",
    "**Example**:\n",
    "\n",
    "* Define sufficient statistic $s = \\sum_i\\mathcal{I}(x_i=1)$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(\\theta| \\mathcal{D}) \n",
    "&\\propto \\theta^{\\tau_0 +s} (1-\\theta)^{\\nu_0- \\tau_0 + n - s} \\\\\n",
    "&= \\theta^{\\tau_n}(1-\\theta)^{\\nu_n - \\tau_n}\n",
    "\\end{align}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
