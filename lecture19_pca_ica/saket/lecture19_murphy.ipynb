{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "$$ \\LaTeX \\text{ command declarations here.}\n",
    "\\newcommand{\\R}{\\mathbb{R}}\n",
    "\\renewcommand{\\vec}[1]{\\mathbf{#1}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# EECS 545:  Machine Learning\n",
    "## Lecture 19:  Unsupervised Learning: PCA and ICA\n",
    "* Instructor:  **Jacob Abernethy**\n",
    "* Date:  March 28, 2016\n",
    "\n",
    "\n",
    "*Lecture Exposition*: Saket\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "This lecture draws from following resources:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classical PCA: Statement of the theorem\n",
    "\n",
    "Suppose we want to find an orthogonal set of $L$ linear basis vectors $w_j \\in \\R^D$ , and the corresponding scores $z_i \\in \\R^L$ , such that we minimize the average **reconstruction error** \n",
    "$$ J(W,Z) = \\frac{1}{N}\\sum_{i=1}^{N} || x_i - Wz_i ||^2 $$\n",
    "\n",
    "subject to constraint that $W$ is orthonormal.\n",
    "\n",
    "The optimal solution is obtained by setting $\\hat W = V_L$, where $V_L$ contains the $L$ eigenvectors with largest eigenvalues of empirical covariance matrix, $\\hat \\Sigma = \\frac{1}{N} \\sum_{i=1}^{N}(x_i - \\bar x) (x_i-\\bar x)^T$\n",
    "\n",
    "(Proof: Murphy, Section 12.2.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"m_1.png\" align=\"middle\">\n",
    "An illustration of PCA where $D = 2$ and $L = 1$. Circles are the original data points, crosses are the reconstructions. The red dot is the data mean. The points are orthogonally projected onto the line.\n",
    "\n",
    "The diagonal line is the vector $w_1$ ; this is called the first principal component or principal direction. The data points $x_i \\in \\R^2$ are orthogonally projected onto this line to get $z_i \\in \\R$. This is the best 1-dimensional approximation to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## EM algorithm for PCA\n",
    "\n",
    "Let $\\tilde Z$ be $L \\times N$ matrix storing the posterior means (low-dimensional representations)\n",
    "\n",
    "Similarly, let $\\tilde X = X^T$ store the original data along its columns. For $\\sigma =0$:\n",
    "\n",
    "** E-step **:\n",
    "Notice that this is just an orthogonal projection of the data.\n",
    "$$ \\tilde Z = (W^TW)^{-1} W^T \\tilde X $$\n",
    "\n",
    "** M-step **\n",
    "Here we exploit the fact that $\\sigma = cov [z_i |x_i , \\theta] = I$ when $\\sigma^2 = 0$\n",
    "$$ \\hat W = [\\sum_i x_i \\mathbb{E}[z_i]^T][\\sum_i \\mathbb{E}[z_i] \\mathbb{E}[z_i]^T]^{-1}$$\n",
    "$$ W = \\tilde X \\tilde Z^T (\\tilde Z \\tilde Z^T)^{-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## EM for PCA\n",
    "\n",
    "example code: https://github.com/sbailey/empca/blob/master/empca.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"m_2.png\" align=\"middle\">\n",
    "\n",
    "Illustration of EM for PCA when $D = 2$ and $L = 1$. Green stars are the original data points, black circles are their reconstructions. The weight vector w is represented by blue line. (a) We start with a random initial guess of w. The E step is represented by the orthogonal projections. (b) We update the rod w in the M step, keeping the projections onto the rod (black circles) fixed. (c) Another E step. The black circles can ’slide’ along the rod, but the rod stays fixed. (d) Another M step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Advantages of EM over eigen-vector methods\n",
    "\n",
    "* EM can be faster. In particular, assuming $N$, $D \\gg L$, the dominant cost of EM is the projection operation in the E step, so the overall time is $O(T L N D)$, where $T$ is the number of iterations.\n",
    "\n",
    "* EM can be implemented in an online fashion, i.e., we can update our estimate of $W$ as the data streams in.\n",
    "\n",
    "* EM can handle missing data in a simple way.\n",
    "\n",
    "* EM can be extended to handle mixtures of PPCA/ FA models.\n",
    "\n",
    "* EM can be modified to variational EM or to variational Bayes EM to fit more complex models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ICA\n",
    "\n",
    "* Consider the following situation. You are in a crowded room and many people are speaking. Your ears essentially act as two microphones, which are listening to a linear combination of the different speech signals in the room. Your goal is to deconvolve the mixed signals into their constituent parts. This is known as the cocktail party problem, and is an example of blind signal separation (BSS), or blind source separation, where “blind” means we know “nothing” about the source of the signals. \n",
    "\n",
    "\n",
    "* Applications \n",
    "    * acoustic signal processing\n",
    "    * analysing EEG and MEG signals\n",
    "    * financial data,\n",
    "    * any other dataset (not necessarily temporal) where latent sources or factors get mixed together in a linear way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## FastICA\n",
    "\n",
    "* An approximate Newton method for fitting ICA models.\n",
    "\n",
    "* Assume all source distributions are known and are the same, so we can just write $G(z) = − log p(z)$. Let $g(z) = \\frac {d}{dz} G(z)$. The constrained objective, and its gradient and Hessian are given by:\n",
    "\n",
    "$$\\begin{align}\n",
    " & f(v) = \\mathbb{E}[G(v^Tx)] + \\lambda (1-v^T v) \\\\\n",
    " & \\nabla f(v) = \\mathbb{E}[xg(v^Tx)] -\\beta v \\\\\n",
    " & H(v) = \\mathbb{E} [xx^T g'(v^T x)]- \\beta I \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $ \\beta = 2 \\lambda$ is lagrange multiplier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Let us make the approximation\n",
    "\n",
    "$$ \\mathbb{E} [xx^T g'(v^T x)]  \\approx \\mathbb{E}[xx^T] \\mathbb{E}[g'(v^T x)] = \\mathbb{E}['g'(v^T x)] $$\n",
    "\n",
    "* This makes the Hessian very easy to invert, giving rise to the following Newton update:\n",
    "\n",
    "$$ v^* := v - \\frac {\\mathbb{E}[xg(v^Tx)] - \\beta v}{\\mathbb{E}[xg'(v^Tx)] - \\beta} $$\n",
    "\n",
    "* $$ v^{new} := \\frac{v*}{|| v* ||}$$"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
