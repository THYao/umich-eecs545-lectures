{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'scroll': False,\n",
       " u'start_slideshow_at': 'selected',\n",
       " u'theme': 'simple',\n",
       " u'transition': 'fade',\n",
       " u'width': 1024}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rise config\n",
    "from notebook.services.config import ConfigManager\n",
    "cm = ConfigManager()\n",
    "cm.update('livereveal', {\n",
    "              'theme': 'simple',\n",
    "              'start_slideshow_at': 'selected',\n",
    "              'transition':'fade',\n",
    "              'scroll':False\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# EECS 545 Winter 2016\n",
    "# Tutorial 4: Optimization Review\n",
    "\n",
    "**Presented by:** Changhan Wang (wangchh@umich.edu)\n",
    "\n",
    "\n",
    "**Credits to: ** Jake, Clayton Scott, Hang Li \n",
    "\n",
    "*Feb 3, 2016*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline\n",
    "\n",
    "* Unconstrained Optimization\n",
    "* Constrained Optimization\n",
    "    * Langrange duality\n",
    "    * KKT conditions\n",
    "    * Convex function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Unconstrained Optimization\n",
    "\n",
    "* ** Differentiable **: $\\forall x$, the gradient $\\nabla f(x)$ exists\n",
    "* ** Twice differentiable **: $\\forall x$, the Hessian matrix $\\nabla^2 f(x)$ exists\n",
    "* **Stationary point**: $x$ where $\\nabla f(x) = \\vec{0}$\n",
    "* ** Saddle point **: a stationary point but not a local minimizer/maximizer\n",
    "    * e.g. $x=0$ for $f(x) = x^3$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* **Problem formulation**\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\min_{x\\in \\mathbb{R}^d} \\quad& f(x)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "* We need to find the stationary points\n",
    "* Global minimizer $\\Rightarrow$ local minimizer $\\Rightarrow$ stationary point\n",
    "* Solving the equation: closed-form solutions, gradient descent, Newton's method, etc.\n",
    "**Stationary point $\\Rightarrow$ local minimizer $\\Rightarrow$ global minimizer?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Finite # of stationary points: check everyone\n",
    "* Differentiable convex function: global minimizer! $$f(y)\\geq f(x)+\\langle\\nabla f(x),y-x\\rangle=f(x)$$\n",
    "* Twice continuous differentiable function: \n",
    "    $$\n",
    "    \\begin{align*}\n",
    "    f(y) &= f(x) + \\langle\\nabla f(x), y-x\\rangle + \\frac{1}{2} \\langle y-x, \\nabla^2 f(x)(y-x) \\rangle \\\\&+ o(\\Vert y-x\\Vert^2)\\end{align*}$$\n",
    "    * $f(x+tv) - f(x) = t^2\\left(\\frac{1}{2}\\langle v, \\nabla^2 f(x)v \\rangle + \\frac{o(t^2)}{t^2}\\right)$ where we let $\\Vert v\\Vert=1$ ($v$ on the unit ball centered at $x$)\n",
    "    * $\\nabla^2 f(x)$ positive definite $\\Rightarrow$ local minimizer, how about PSD?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* ** Property**: differentiable, local minimizer $x^*$ $\\Rightarrow$ $\\nabla f(x) = \\vec{0}$ (*the inverse is not true*)\n",
    "* ** Property**: twice differentiable, local minimizer $x^*$ $\\Rightarrow$ $\\nabla^2 f(x)$ is PSD (*the inverse is not true*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Constrained Optimization\n",
    "\n",
    "* **Problem formulation**\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\min_{x\\in \\mathbb{R}^d} \\quad& f(x) \\\\\n",
    "\\text{s.t.} \\quad& g_i(x) \\leq 0, \\quad i = 1, \\ldots, m\\\\\n",
    " & h_j(x) = 0, \\quad j = 1, \\ldots, n\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "* The set $C=\\{ x\\vert g_i(x) \\leq 0, h_j(x) = 0, i=1, \\ldots, m, j=1, \\ldots, n \\}$ is convex if $g_i(x)$ convex and $h_j(x)$ affine\n",
    "* The solution of this optimization may occur in the *interior* of $C$, in which case the optimal $x$ will have $\\nabla f(x) = 0$\n",
    "* But what if the solution occurs on the *boundary* of $C$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Langrange Duality\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\min_{x\\in \\mathbb{R}^d} \\quad& f(x) \\\\\n",
    "\\text{s.t.} \\quad& g_i(x) \\leq 0, \\quad i = 1, \\ldots, m\\\\\n",
    "& h_j(x) = 0, \\quad j = 1, \\ldots, n\n",
    "\\end{align*}\n",
    "$$\n",
    "* **Lagrangian function**\n",
    "$$\n",
    "L(x,\\boldsymbol{\\lambda}, \\boldsymbol{\\nu}) := f(x) + \\sum_{i=1}^m \\lambda_i g_i(x) + \\sum_{j=1}^n \\nu_j h_j(x)\n",
    "$$\n",
    "* **Lagrange multipliers/dual variables**: $\\boldsymbol{\\lambda} \\in \\mathbb{R}^m$ and $\\boldsymbol{\\nu} \\in \\mathbb{R}^n$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* $$\n",
    "L(x,\\boldsymbol{\\lambda}, \\boldsymbol{\\nu}) := f(x) + \\sum_{i=1}^m \\lambda_i g_i(x) + \\sum_{j=1}^n \\nu_j h_j(x)\n",
    "$$\n",
    "\n",
    "\n",
    "* **Primal function**\n",
    "$$\n",
    "\\begin{align*}\n",
    "L_{P}(x)=\n",
    "\\left\\{\\begin{array}{r} \\max_{\\boldsymbol{\\lambda}, \\boldsymbol{\\nu}}\\, L(x,\\boldsymbol{\\lambda}, \\boldsymbol{\\nu}) \\\\ \\text{s.t.}\\quad \\lambda_i\\geq 0\\end{array}\\right.\n",
    "=\\left\\{\\begin{array}{ll}f(x) & x\\in C\\\\ +\\infty & x\\notin C \\end{array}\\right.\n",
    "\\end{align*}\n",
    "$$\n",
    "where $ C = \\{ x\\vert g_i(x) \\leq 0, h_j(x) = 0, i=1, \\ldots, m, j=1, \\ldots, n \\}$\n",
    "\n",
    "\n",
    "* What's the difference between $f(x)$ and $L_P(x)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Hence the original optimization is equivalent to the **primal problem**:\n",
    "$$\\min_{x\\in \\mathbb{R}^n} L_P(x)$$\n",
    "or\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\min_{x\\in \\mathbb{R}^d} \\max_{\\boldsymbol{\\lambda}\\in \\mathbb{R}^m, \\boldsymbol{\\nu}\\in \\mathbb{R}^n}\\, &L(x,\\boldsymbol{\\lambda}, \\boldsymbol{\\nu})\\\\\n",
    "\\text{s.t.}\\quad &\\lambda_i \\geq 0\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "* It is actually a double optimization (inner optimization in exchange of unconstrained $x$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Swap the outer and inner optimization to get the **dual problem**:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\max_{\\boldsymbol{\\lambda}\\in \\mathbb{R}^m, \\boldsymbol{\\nu}\\in \\mathbb{R}^n}\\min_{x\\in \\mathbb{R}^d}\\, &L(x,\\boldsymbol{\\lambda}, \\boldsymbol{\\nu})\\\\\n",
    "\\text{s.t.}\\quad &\\lambda_i \\geq 0\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "** Dual function **\n",
    "$$L_D(\\boldsymbol{\\lambda}\\in \\mathbb{R}^m, \\boldsymbol{\\nu}\\in \\mathbb{R}^n)=\\min_{x\\in \\mathbb{R}^d}\\, L(x,\\boldsymbol{\\lambda}, \\boldsymbol{\\nu})$$\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What's the connection between primal and dual problem?\n",
    "* Primal solution: $x^*$, dual solution: $\\lambda^*$ and $\\nu^*$\n",
    "\n",
    "* **Weak duality** (always true)\n",
    "$$d^*=L_D(\\boldsymbol{\\lambda}^*, \\boldsymbol{\\nu}^*)\\leq p^*=L_P(x^*)=f(x^*)$$\n",
    "\n",
    "* **Strong duality** (under additional conditions): $d^* = p^* =f(x^*)= L(x^*,\\boldsymbol{\\lambda}^*, \\boldsymbol{\\nu}^*)$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "L(x^*,\\boldsymbol{\\lambda}^*, \\boldsymbol{\\nu}^*)\n",
    "\\leq p^*=\\max_{\\boldsymbol{\\lambda}, \\boldsymbol{\\nu}}\\, L(x^*,\\boldsymbol{\\lambda}, \\boldsymbol{\\nu})\n",
    "\\leq d^*=\\min_x\\, L(x,\\boldsymbol{\\lambda}^*, \\boldsymbol{\\nu}^*)\\leq L(x^*,\\boldsymbol{\\lambda}^*,\\boldsymbol{\\nu}^*)\n",
    "\\end{align*}\n",
    "$$\n",
    "* Sometimes the dual problem is easier. But when strong duality holds?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Karush–Kuhn–Tucker (KKT) conditions\n",
    "* Suppose differentiable $f(x),g_i(x),h_j(x)$\n",
    "* (Primal feasibility) $$g_i(x)\\leq 0, h_j(x)=0$$\n",
    "* (Dual feasibility) $$\\lambda_i\\geq 0$$\n",
    "* (Stationarity) $$\\nabla_x L(x, \\boldsymbol{\\lambda}, \\boldsymbol{\\nu})=0$$\n",
    "* (Complementary slackness) $$\\lambda_i g_i(x)=0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Necessary conditions of strong duality\n",
    "* Strong duality $\\Rightarrow$ KKT conditions hold for $x^*$, $\\lambda^*$ and $\\nu^*$ (Proof?)\n",
    "\n",
    "\n",
    "### Sufficient conditions of strong duality\n",
    "* KKT conditions hold for $\\tilde{x}$, $\\tilde{\\boldsymbol{\\lambda}}$ and $\\tilde{\\boldsymbol{\\nu}}$ $\\Rightarrow$ strong duality, primal solution $\\tilde{x}$, dual solution $\\tilde{\\boldsymbol{\\lambda}}$ and $\\tilde{\\boldsymbol{\\nu}}$ (Proof?)\n",
    "* For convex problem: $f(x)$ and $g_i(x)$ are convex, $h_j(x)$ are affine\n",
    "* Slater's conditions: $\\exists x$ s.t. $g_i(x)<0$ and $h_j(x)=0$\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Convex Function\n",
    "\n",
    "#### For unconstrained optimization\n",
    "\n",
    "* **Stationary point $\\Rightarrow$ global minimizer**\n",
    "* **Uniqueness of global minimizer for strictly convex function** (HW1 Q5)\n",
    "\n",
    "#### For constrained optimization\n",
    "* For convex problem\n",
    "    * Use Slater's conditions to get strong duality\n",
    "    * Use KKT conditions to get strong duality and find the solutions"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
