{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "$$ \\LaTeX \\text{ command declarations here.}\n",
    "\\newcommand{\\R}{\\mathbb{R}}\n",
    "\\renewcommand{\\vec}[1]{\\mathbf{#1}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# plotting\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt;\n",
    "\n",
    "# scientific\n",
    "import numpy as np;\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.learning_curve import learning_curve\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "# math\n",
    "from __future__ import division\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# EECS 545:  Machine Learning\n",
    "## Lecture 10:  Bayesian Linear Regression and Gaussian Processes\n",
    "* Instructor:  **Jacob Abernethy**\n",
    "* Date:  February 11, 2016\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Linear Gaussian Distributions\n",
    "\n",
    "* Linear combination of Gaussian random variable also has a Gaussian distribution.\n",
    "\n",
    "\n",
    "    - Its marginal distribution is also a Gaussian.\n",
    "    - Its conditional distribution is also a Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Bayes’ Theorem for Gaussian Variables\n",
    "\n",
    "* Given:\n",
    "$$ p(x) = \\mathcal{N}(x|\\mu , \\Lambda^{-1}) $$\n",
    "$$ p(y|x) = \\mathcal{N}(y|Ax+b , L^{-1}) $$\n",
    "\n",
    "* we have:\n",
    "$$ p(y) = \\mathcal{N}(y|A\\mu + b , L^{-1} + A\\Lambda^{-1}A^T) $$\n",
    "$$ p(x|y) = \\mathcal{N}(x|\\Sigma\\{A^TL(y-b) + \\Lambda\\mu\\} , \\Sigma) $$\n",
    "\n",
    "where $ \\Sigma = (\\Lambda + A^TLA)^{-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bayesian Linear Regression\n",
    "\n",
    "## What is Regression?\n",
    "\n",
    "* Given a set of observations: $ x = \\{ x_1 . . . x_N \\}$ and the corresponding target values: $ t = \\{ t_1 . . . t_N \\} $\n",
    "\n",
    "\n",
    "\n",
    "* We want to learn a function $y(x)=t$ to predict future values. \n",
    "    - We have just learned to find the maximum likelihood weights $w_{ML}$ , to predict $y(x,w_{ML})$.\n",
    "    \n",
    "    $$ w_{ML} = (\\lambda I + \\Phi^T\\Phi)^{-1} \\Phi^T t $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overview: Bayesian Linear Regression\n",
    "\n",
    "* With a likelihood:$ p(t|X,w,\\beta) = \\prod_{n=1}^{N} \\mathcal{N}(t_n|w^T \\phi(x_n), \\beta^{-1}) $ \n",
    "\n",
    "* and a prior: $ p(w) = \\mathcal{N}(w|m_0, S_0) $\n",
    "\n",
    "\n",
    "* We can get a posterior as:\n",
    "$$ p(w|t) = \\mathcal{N}(w|m_N, S_N) $$\n",
    "where: $m_N = S_N(S_0^{-1}m_0 + \\beta \\Phi^Tt)$ and $S_N^{-1} = S_0^{-1} + \\beta\\Phi^T\\Phi $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simplifying the Prior\n",
    "\n",
    "* Zero-mean isotropic Gaussian:\n",
    "$$ p(w|\\alpha) = \\mathcal{N}(w|0, \\alpha^{-1}I) $$\n",
    "\n",
    "\n",
    "* The corresponding posterior is:\n",
    "$$ p(w|t) = \\mathcal{N}(w|m_N, S_N) $$\n",
    "\n",
    "\n",
    "\n",
    "* where $m_N = \\beta S_N\\Phi^Tt$ and $S_N^{-1} = \\alpha I + \\beta \\Phi^T\\Phi $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Derivation of posterior distribution\n",
    "\n",
    "* Use linear Gaussian distributions:\n",
    "$$ p(w) = \\mathcal{N}(w|0, \\alpha^{−1}I)$$\n",
    "$$ p(t|w,x) = \\mathcal{N}(t|\\phi w, \\beta^{−1}I) $$\n",
    "\n",
    "\n",
    "* Conditional distribution:\n",
    "$$ p(w|t,x) = \\mathcal{N}(w|\\Sigma(\\beta\\Phi^Tt), \\Sigma^{−1}) $$\n",
    "\n",
    "where $ \\Sigma = (\\alpha I + \\beta \\Phi^T \\Phi)^{-1} $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recall\n",
    "\n",
    "$$ \n",
    "\\begin{array}{rcl}\n",
    "p(x) &= &\\mathcal{N}(x|\\mu, \\Lambda^{-1})\\\\\n",
    "p(y|x) &= &\\mathcal{N}(y|Ax+b, L^{-1})\\\\\n",
    "p(y) &= &\\mathcal{N}(y|A\\mu + b , L^{-1} + A\\Lambda^{-1}A^T)\\\\\n",
    "p(x|y) &= &\\mathcal{N}(x|\\Sigma\\{A^TL(y-b) + \\Lambda\\mu\\} , \\Sigma)\\\\\n",
    "\\Sigma &= &(\\Lambda + A^TLA)^{-1}\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recall: Maximum Likelihood\n",
    "\n",
    "* Same as minimizing sum-of-squared error, with regularization term:\n",
    "$$ ln p(w|t) = - \\frac{\\beta}{2}\\sum_{n=1}^{N}\\{ t_n - w^T\\phi(x_n)\\}^2 - \\frac{\\alpha}{2}w^Tw + constant $$\n",
    "\n",
    "\n",
    "* This is the same as $w_{ML}$ , with $ \\lambda = \\frac{\\alpha}{\\beta}$\n",
    "\n",
    "\n",
    "* So the solution is (same as $w_{ML}$):\n",
    "$$ w_{MAP} = (\\lambda I + \\Phi^T \\Phi)^{-1}\\Phi^Tt $$\n",
    "\n",
    "* But now we have the variance on w, as well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sequential Bayesian Learning\n",
    "\n",
    "* Simple model: $y(x,w) = w_0 + w_1 x$\n",
    "\n",
    "** Posterior  = Prior * Likelihoood **\n",
    "<img src = \"sbl.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sequential Bayesian Learning\n",
    "\n",
    "* Samples drawn from posterior\n",
    "\n",
    "$\\hspace{5em}$likelihood (for a given example) $\\hspace{8em}$ posterior $\\hspace{12em}$ data\n",
    "<img src = \"sbl2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sequential Bayesian Learning\n",
    "\n",
    "* Sample lines drawn from posterior.\n",
    "\n",
    "$\\hspace{5em}$likelihood (for a given example) $\\hspace{8em}$ posterior $\\hspace{12em}$ data\n",
    "<img src = \"sbl3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sequential Bayesian Learning\n",
    "\n",
    "* Sample lines drawn from posterior\n",
    "\n",
    "$\\hspace{5em}$likelihood (for a given example) $\\hspace{8em}$ posterior $\\hspace{12em}$ data\n",
    "<img src = \"sbl4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Predictive Distribution\n",
    "\n",
    "* Our real goal is to predict t given new x, so we evaluate the predictive distribution:\n",
    "$$ p(t|x,{\\bf t}, \\alpha, \\beta) = \\int p(t|x,w \\beta) p(w|{\\bf t}, \\alpha, \\beta) dw $$\n",
    "\n",
    "\n",
    "* where $ p(t|x,w,\\beta) = \\mathcal{N}(t|y(x,w), \\beta^{-1} $ and we just derived $ p(w|{\\bf t}) = \\mathcal{N}(w|m_N, S_N)$, $m_N = \\beta S_N \\Phi^Tt$ and $S_N^{-1} = \\alpha I + \\beta \\Phi^T \\Phi$\n",
    "\n",
    "\n",
    "* Note: This is equal to $W_MAP^*$, the optimal solution of regularized linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Predictive Distribution\n",
    "\n",
    "* The integral is a convolution of Gaussians, so the result is:\n",
    "$$ p(t_x,{\\bf t}, \\alpha, \\beta) = \\mathcal{N}(t|m_N^T\\phi(x), \\sigma_N^2(x)) $$ \n",
    "\n",
    "\n",
    "* where $ \\sigma_N^2(x) = \\frac{1}{\\beta}+\\phi(x)^TS_n\\phi(x)$\n",
    "    - Intuitively, this corresponds to noise in the data + uncertainty in $w$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Predictive Distribution & Samples\n",
    "\n",
    "* Using 9 Gaussian basis functions $ \\phi_j(x) = exp\\left\\{ -\\frac{(x- \\mu_j)}{2s^2} \\right\\}$\n",
    "\n",
    "<img src = \"n1.png\" height = 200px, width = 300px> <img src = \"n1b.png\" height = 200px, width = 300px>\n",
    "$\\hspace{20em}N=1$ observed point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "<img src = \"n2.png\" height = 200px, width = 300px> <img src = \"n2b.png\" height = 200px, width = 300px>\n",
    "$\\hspace{20em}N=2$ observed points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<img src = \"n4.png\" height = 200px, width = 300px> <img src = \"n4b.png\" height = 200px, width = 300px>\n",
    "$\\hspace{20em}N=4$ observed points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "<img src = \"n25.png\" height = 200px, width = 300px> <img src = \"n25b.png\" height = 200px, width = 300px>\n",
    "$\\hspace{20em}N=25$ observed points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gaussian Processes\n",
    "\n",
    "### Why?\n",
    "* Here are some data points. What function did they come from?\n",
    "<img src=\"gp1.png\">\n",
    "\n",
    "\n",
    "* GPs are a nice way of expressing this “prior on functions” idea.\n",
    "\n",
    "\n",
    "* Applications: Regression and Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Definition of GP\n",
    "\n",
    "* A Gaussian process is defined as a **probability distribution over functions y(x)**, such that the set of values of y(x) evaluated at an arbitrary set of points $x_1 , x_2 , ... , x_n$ jointly have a **Gaussian distribution**.\n",
    "    - Any finite subset of indices defines a multivariate Gaussian distribution (i.e., $(y(x_1 ), y(x_2 ), ... , y(x_n )$) is a multivariate Gaussian.\n",
    "    \n",
    "\n",
    "* What determines the GP is\n",
    "    - The mean function $\\mu(x) = \\mathop{\\mathbb{E}}(y(x))$\n",
    "    - The covariance function (kernel) $k(x,x')=\\mathop{\\mathbb{E}}(y(x)y(x'))$\n",
    "    - In most applications, we take $\\mu(x)=0$. Hence the prior is represented by the kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Covariance function of GP defines prior\n",
    "\n",
    "* The figure show samples of functions drawn from Gaussian processes for two different choices of kernel functions.\n",
    "\n",
    "\n",
    "<img src=\"gpleft.png\">\n",
    "$$k(x, x') = exp(−\\theta ||x − x'||_2^2) \\hspace{4em} k(x, x') = exp(−\\theta ||x − x'||_1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Regression Revisited\n",
    "\n",
    "* Linear regression model: Combination of M fixed basis functions given by $\\phi(x)$, so that \n",
    "$$ y(x) = w^T \\phi(x) $$\n",
    "\n",
    "\n",
    "* Prior distribution: $ p(w) = \\mathcal{N}(w|0, \\alpha^{-1}I) $\n",
    "\n",
    "\n",
    "* Given training data points $(x_1 , x_2 , ... , x_n)$ , what is the joint distribution of $ y(x_1) , y(x_2) , ..., y(x_n)$?\n",
    "    - y is the vector with elements $y_i = y(x_i)$ , which is given by $$ {\\bf y = \\Phi w }$$ \n",
    "    - where $\\Phi$ is the design matrix with elements $\\Phi_{nk} = \\Phi_k(x_n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Regression Revisited\n",
    "\n",
    "* $y = \\Phi w$:- y is a linear combination of Gaussian distributed variables w, hence itself is Gaussian.\n",
    "\n",
    "* Mean and covariance:\n",
    "$$ \\mathop{\\mathbb{E}}[y] = \\Phi\\mathop{\\mathbb{E}}[w] = 0 $$\n",
    "$$ Cov(y)  =\\mathop{\\mathbb{E}}[yy^T] = \\Phi\\mathop{\\mathbb{E}}[ww^T]\\Phi^T = \\frac{1}{\\alpha}\\Phi\\Phi^T = K $$\n",
    "\n",
    "\n",
    "* K is the gram matrix with elements $K_{nm} = \\kappa(x_n,x_m) = \\frac{1}{\\alpha} \\Phi(x_n)^T\\Phi(x_m)$ and $\\kappa(x,x')$ is the kernel function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bayesian Linear Regression and GP\n",
    "\n",
    "* In summary, Bayesian linear regression is a special instance of a Gaussian Process.\n",
    "\n",
    "\n",
    "\n",
    "* It is defined by the linear regression model $y(x) = w^T\\phi(x)$ with a weight prior $p(w)=\\mathcal{N}(w|0, \\alpha^{-1} I)$\n",
    "\n",
    "\n",
    "* The kernel function is given by $\\kappa(x_n, x_m) = \\frac{1}{\\alpha}\\phi(x_n)^T\\phi(x_m)$\n",
    "\n",
    "\n",
    "* Features in Bayesian LR $\\iff$ kernel functions in GP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GP for regression\n",
    "\n",
    "* Consider the noise on the observed target values $t_n = y_n + \\epsilon_n$ where $y_n = y(x_n)$ and $\\epsilon_n $ is a random noise.\n",
    "\n",
    "\n",
    "* Equivalently, consider a noise process: $p(t_n|y_n) = \\mathcal{N}(t_n|y_n, \\beta^{-1})$ where where $\\beta$ is a hyperparameter (precision of the noise).\n",
    "\n",
    "\n",
    "* Since $\\epsilon_n$ independent, this can be represented as multivariate Gaussian\n",
    "$$ p({\\bf t|y}) = \\mathcal{N}({\\bf t}|{\\bf y}, \\beta^{-1} {\\bf I}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GP for regression\n",
    "\n",
    "* From the definition of GP, the marginal distribution p(y) is given by $p(y) = \\mathcal{N}(y|0,K)$.\n",
    "    - Note: $p(y)$ and $p(t|y)$ forms linear Gaussian distribution\n",
    "\n",
    "\n",
    "* Then, the marginal distribution of t is given by \n",
    "$$p(t) = \\int p(t|y)p(y)dy = \\mathcal{N}(t|0, C) $$ \n",
    "    - where the covariance matrix C has elements \n",
    "$$ C(x_n, x_m) = \\kappa(x_n,x_m) + \\beta^{-1}\\delta_{mn} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: sampling data points\n",
    "\n",
    "* Sample function from GP (blue): $y(x) ~ GP(\\mu, K)$\n",
    "\n",
    "\n",
    "* Sample points from GP (red): $(x_1 , y (x_1)) , (x_2 , y(x_2)) ,..., (x_N , y(x_N)) ~ GP(\\mu, K)$\n",
    "\n",
    "\n",
    "* Add noise (green): $t_n(x) ~ y_n(x) + \\mathcal{N}(0, \\beta^{-1} )$\n",
    "\n",
    "<img src =\"gpexample.png\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GP for regression\n",
    "\n",
    "* We have used GP to build a model of the joint distribution over sets of data points.\n",
    "\n",
    "\n",
    "* **Goal**: Given data $(x_1 , ... , x_n)$ and target values $t = (t_1 , ... , t_n)$ , predict $t_{n+1}$ for query point $x_{n+1}$ .\n",
    "\n",
    "\n",
    "* Idea: GP assumes that $p(t_1 , ... , t_n, t_{n+1} ~ \\mathcal{N}(0, C_{n+1} )$ where \n",
    "        - $C_{n+1}$ is $(n+1)\\times(n+1)$ matrix $C(x_n,x_m) = \\kappa(x_n, x_m) + \\beta^{-1}\\delta_{nm}$\n",
    "        \n",
    "        \n",
    "* $C_{n+1} = \\left\\{\\begin{array}{cc} C_n & k \\\\ k^T & c \\end{array}\\right\\}$ where $C_n$ is $n \\times n$ matrix, and $c = k(x_{n+1} ,x_{n+1})+ \\beta^{-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GP for regression\n",
    "\n",
    "* The conditional distribution $p(t_{n+1} |t)$ is a Gaussian distribution with mean and covariance given by:\n",
    "$$ m(x_{n+1}) = k^T C_n^{-1}t \\\\ \\sigma^2(x_{n+1}) = c- k^TC_n^{-1} k$$\n",
    "\n",
    "\n",
    "\n",
    "* These are the key results that define Gaussian process regression.\n",
    "\n",
    "\n",
    "* The predictive distribution is a Gaussian whose **mean and variance both depend** on $x_{n+1}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## An Example of GP regression\n",
    "\n",
    "* Green: underlying function (sine function)\n",
    "* Blue: samples from GP (with noise)\n",
    "* Red: prediction from GP regression ) with “error bars”\n",
    "\n",
    "<img src = \"gp_regression.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GP for Regression\n",
    "\n",
    "* The only restriction on the kernel is that the covariance matrix given by $C(x_n, x_m) = \\kappa(x_n, x_m) + \\beta^{-1}\\delta_{nm}$ must be positive definite.\n",
    "\n",
    "\n",
    "\n",
    "* GP will involve a matrix of size $N \\times N$, for which require $O(N^3)$ computations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Learning Hyperparameters\n",
    "\n",
    "* Log likelihood: \n",
    "$$ ln(p(t|\\theta) = -\\frac{1}{2} ln|C_N| - \\frac{1}{2}t^TC_N^{-1}t - \\frac{N}{2} ln(2\\pi) $$\n",
    "\n",
    "\n",
    "* Gradient Ascent for parameter $\\theta$:\n",
    "$$ \\frac{\\partial}{\\partial{\\theta_i}}lnp(t|\\theta) = c-\\frac{1}{2}Tr(C_N^{-1})\\frac{\\partial{C_N}}{\\partial{\\theta_i}} + \\frac{1}{2}t^TC_N^{-1}\\frac{\\partial{C_N}}{\\partial{\\theta_i}}C_N^{-1}t $$\n",
    "\n",
    "\n",
    "* where we have used the following properties:\n",
    "$$\\frac{\\partial{A^{-1}}}{\\partial{x}} = -A^{-1}\\frac{\\partial{A}}{\\partial{x}}A^{-1}$$\n",
    "$$ \\frac{\\partial{ln|A|}}{\\partial{x}} = Tr A^{-1} \\frac{\\partial{A}}{\\partial{x}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary of GP\n",
    "\n",
    "* Distribution over functions\n",
    "\n",
    "*  GP generates data points that are jointly a Gaussian distribution\n",
    "\n",
    "\n",
    "* Most interesting structure is in $\\kappa(x,x’)$, the kernel.\n",
    "\n",
    "\n",
    "* GP can be used for regression to predict the target for a new input example."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
